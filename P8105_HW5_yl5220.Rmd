---
title: "P8105_HW5_yl5220"
output: md_document
date: "2022-11-16"
--- 

```{r}
library(tidyverse)
library(p8105.datasets)
library(viridis)
library(purrr)
knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

# Problem 2
# Describe the raw data
The raw data contain 'r nrow(raw_data)' rows and 'r ncol(raw_data)' of variables, which are 'r colnames(raw_data)'. The variables provide information about the date of the report of the homicide, their names, race, age, sex, which city and state they are from with both names and latitudes and longitudes provided, and their disposition. 

```{r}
raw_data = read_csv("./data/homicide-data.csv")

# Create a city_state variable
homicides_data = 
  raw_data %>% 
  janitor::clean_names() %>% 
  mutate(city_state = str_c(city, ", ", state)) %>% 
  relocate(city_state, .after = reported_date)

homicides_data


# Summarize within cities to obtain the total number of homicides and the number of unsolved homicides (those for which the disposition is “Closed without arrest” or “Open/No arrest”).
unique(homicides_data$disposition)

homicides_data %>% 
  group_by(city_state) %>% 
  summarize(
    total_homicides = n(),
    unsolved_homicides = sum(disposition == "Closed without arrest") + sum(disposition == "Open/No arrest")
  ) 
  


# For the city of Baltimore, MD, use the prop.test function to estimate the proportion of homicides that are unsolved, save the output of prop.test as an R object, apply the broom::tidy to this object and pull the estimated proportion and confidence intervals from the resulting tidy dataframe.

baltimore_homicide = 
  homicides_data %>% 
  filter(city_state == "Baltimore, MD") %>% 
  group_by(city_state) %>% 
  summarize(
    bal_total_homicides = n(),
    bal_unsolved_homicides = sum(disposition == "Closed without arrest") + sum(disposition == "Open/No arrest")
  )


baltimore_homicide_test = 
  prop.test(
    x = baltimore_homicide %>% pull(bal_unsolved_homicides),
    n = baltimore_homicide %>% pull(bal_total_homicides)
  )

baltimore_homicide_test

tidied_test = 
  baltimore_homicide_test %>% 
  broom::tidy()
tidied_test

bal_estimate = tidied_test %>% pull(estimate)
bal_lower_CI = tidied_test %>% pull(conf.low)
bal_upper_CI = tidied_test %>% pull(conf.high)

bal_estimates_data = list(
                          "Estimated_Proportion" = bal_estimate,
                          "Lower_CI_Bound" = bal_lower_CI,
                          "Upper_CI_Bound" = bal_upper_CI
)

bal_estimates_data

# Run prop.test for each of the cities in your dataset, and extract both the proportion of unsolved homicides and the confidence interval for each

# Create the function for getting the proportion of unsolved homicides and the confidence interval for each city from prop.test
proptest = function(citystate_name) {
  city_data = 
    homicides_data %>% 
    filter(city_state == citystate_name) %>% 
    group_by(city_state) %>% 
    summarize(
    city_total_homicides = n(),
    city_unsolved_homicides = sum(disposition == "Closed without arrest") + sum(disposition == "Open/No arrest")
    )
  
  city_prop_test = 
    prop.test(
    x = city_data %>% pull(city_unsolved_homicides),
    n = city_data %>% pull(city_total_homicides)
  ) %>% 
    broom::tidy() %>% 
    select(estimate, conf.low, conf.high)
  
  city_prop_test
}


city_state_name = unique(homicides_data$city_state)

city_prop_test = 
  expand_grid(city_state = city_state_name) %>% 
  mutate(test_results = map(city_state, proptest)) %>% 
  unnest(test_results)

city_prop_test

n = unique(city_prop_test$city_state)
n


# Create a plot that shows the estimates and CIs for each city – check out geom_errorbar for a way to add error bars based on the upper and lower limits. Organize cities according to the proportion of unsolved homicides.
city_prop_test %>% 
  mutate(city_state = reorder(city_state, estimate)) %>% 
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() +
  labs(
        title = "The Estimated Proportion and Confidence Interval for Each City",
        x = "City and State",
        y = "Estimated Proportion and 95% Confidence Interval"
            ) +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +  
  theme(axis.text.x = element_text(angle = 90, vjust = 0.8, hjust=1))
  theme(legend.position = "bottom") 
```


# Problem 3
```{r}
# Set μ=0. Generate 5000 datasets from the model
new_data = rerun(5000, rnorm(n = 30, mean = 0, sd = 5))

# For each dataset, save μ̂  and the p-value arising from a test of H:μ=0 using α=0.05. Hint: to obtain the estimate and p-value, use broom::tidy to clean the output of t.test
t_test = function(mu = 0){
  data_t_test = tibble(rnorm(n = 30, mean = mu, sd = 5))
  
  results = 
    t.test(data_t_test) %>% 
    broom::tidy() %>% 
    select(estimate, p.value)
  results
}

t_test()

mu_is_0 = 
  expand.grid(true_mean = 0, iter = 1:5000) %>% 
  mutate(test_results = map(true_mean, t_test)) %>% 
  unnest(test_results)

head(mu_is_0)


# Repeat the above for  μ={1,2,3,4,5,6}
mu_not_0 = 
  expand.grid(true_mean = 1:6, iter = 1:5000) %>% 
  mutate(test_results = map(true_mean, t_test)) %>% 
  unnest(test_results)

head(mu_not_0)


# Make a plot showing the proportion of times the null was rejected (the power of the test) on the y axis and the true value of μ on the x axis. Describe the association between effect size and power.
mu_not_0 %>% 
  filter(p.value < 0.05) %>% 
  group_by(true_mean) %>% 
  summarize(
    power_of_test = n() / 5000
  ) %>% 
  ggplot(aes(x = true_mean, y = power_of_test)) +
  geom_point() +
  geom_smooth()
  labs(
        title = "The association between effect size and power",
        x = "True Value of mu",
        y = "The Power of the Test"
            )
```
Based on the plot, the power of the test increases with the effect size until the effect size reached the value of one generally.

```{r}
# Make a plot showing the average estimate of μ̂  on the y axis and the true value of μ on the x axis
mu_not_0 %>% 
  group_by(true_mean) %>% 
  summarize(
    average_estimate = mean(estimate)
  ) %>% 
  ggplot(aes(x = true_mean, y = average_estimate)) +
  geom_point() +
  geom_smooth()
  labs(
        title = "The association between the true mean and estimated average mean",
        x = "True Mean",
        y = "The Average Estimated of Mean"
            )
  
  
# Make a second plot (or overlay on the first) the average estimate of μ̂  only in samples for which the null was rejected on the y axis and the true value of μ on the x axis.Is the sample average of μ̂  across tests for which the null is rejected approximately equal to the true value of μ? Why or why not?
mu_not_0 %>% 
  filter(p.value < 0.05) %>% 
  group_by(true_mean) %>% 
  summarize(
    average_estimate = mean(estimate)
  ) %>% 
  ggplot(aes(x = true_mean, y = average_estimate)) +
  geom_point() +
  geom_smooth()
  labs(
        title = "The association between the true mean and estimated average mean for which H0 was rejected",
        x = "True Mean",
        y = "The Average Estimated of Mean for which H0 was rejected"
            )
```
Based on the two plots, it can be seen that the sample average of μ̂  across test for which the null is rejected
is not approximately equal to the true value of μ, because under the situation when H0 is rejected, that is, when p_value is less than 0.05, there is little possibility for the sample mean to be equal to the true mean.
```
